CS156

definition of AI in class; representation scheme, a search scheme, and a reasoning scheme.
	- searching, representing, reasoning

intelligent agents/systems must have
1) representation - 

wolf, goat, cabbage problem (cross river problem)
- can store information into a data structure 

- kinda like 5D chess where there are numerous outcomes that lead to more

2) search

# of planets in universe ~ 10^20
# of atoms in universe ~ 10^50
# of possible chess moves ~ 10^110 (legal moves) <-> 10^123 (illegal moves)
	- there must be some sort of algorithm or decision tree where they focus on only the useful/needed moves

- there must be some sort of search scheme that lets you find acceptable choices in a reasonable time

3) reasoning - 

imagine you have 2 buckets full of coins, run up and down the stairs, what happens to the coins? they spill out.
	- how did you figure it out? you never did this action before? 
		--> you made an analogy to an experience you had

- there must be some sort of reasoning base; deductive, inductive, something to infer what could happen

setup python environment


min-max trees and related algo
- many states with possible moves being the subsequent level (each level is a state of the environment)
  e.g. chess
	- since there is an absurd amt of possible chess moves, you only go down the tree to a certain depth

Agents and Environments --------------------
- the agent function maps from percept histories to actions [f: P* -> A]
- the agent program runs on the physical architecture to produce f
- agent = architecture + program

The PAGE Process (for designing an agent) --------------------
- PAGE = percepts, actions, goal, environment
- tasks for designing an AGENT-BASED systems
	What will the agent need to perceive?
	What actions will the agent need to perform?
	What goals will the agent need or want to accomplish? ("aim" is what you want to achieve, goal is how you achieve the "aim")
	What will be the environment in which the agent will operate?

	PAGE example (automated TAXI) --------------------
	Perceptions? - location, destination, traffic, other cars, pedestrians, traffic lights, etc.
	Actions? - accelerate, brake, turn, honk, etc.
	Goals? - get to destination, obey traffic laws, etc.
	Environment? - roads, other cars, pedestrians, traffic lights, etc.

PAGE exercise (Financial Planning Agent) --------------------
	Perceptions? current financial situation, current trends on stock markets, news channels, political climate, etc
	Actions? invest, save, purchase assets (e.g. housing, land, etc.), etc
	Goals? achieve financial stability, retirement, financial goals, etc
	Environment? emails, financial instituation dashboards, financial situations (interest rate, supply, demand), etc

--------------------------------------------
Example: Romania
- go on holiday in Romania, currently in Arad
- flight leaves tomorrow from Bucharest

Formulate Goal
	- be in Bucharest

Formulate Problem
	- states: various cities
	- actions: drive between cities
	- initial state: Arad
	- goal state: Bucharest

Find Solution
	- sequence of cities: Arad -> Sibiu -> Fagaras -> Bucharest

--------------------------------------------

Problem types
- deterministic, fully observable --> single-state problem
	: agent knows exactly which state it will be in; solution is a sequence
- deterministic, non-observable --> sensorless problem (conformat problem)
	: agent may have no idea where it is; solution is a sequence
- nondeterministic and/or partially observable --> contingency problem
	: percepts provide new information abt current state; often interleave search, execution
- unknown state space --> exploration problem
	: agent must gather data to find solution (as executing, take in new information and use it to find/adjust solution)

Tree-Search algorithm
- function TREE-SEARCH(problem, strategy) returns a solution, or failure
	- initialize the search tree using the initial state of problem
	- loop do
		- if there are no candidates for expansion then return failure
		- choose a leaf node for expansion according to strategy
		- if the node contains a goal state then return the corresponding solution
		- else expand the node and add the resulting nodes to the search tree

--------------------------------------------
Notion of Rational Agent
- an agent is an entity that perceives and acts
- rational action: whiever action maximizes the performance metric
- rationality is distinct from:
	: omniscience (knowing everything)
	: omnipotence (being able to do anything)
- agents should strive to "do the right thing", based on what they know and have perceived
	: a rational agent should do whatever it can to maximize the performance metric
		- however, the agent must do this within expectation (e.g. travel to destination but not at expense of human life, etc.)

(Performance Metric v Goal ------------------------------------------------------------------------------)
| Performance Metric - how is the agent going to reach the goal (e.g. increase portfolio val by 5% a yr) |
| Goal - what is the agent trying to achieve (e.g. financial stability)									 |
(--------------------------------------------------------------------------------------------------------)

Task Environment
: Fully observable - agent's sensors give it access to the complete state of the environment 
	- e.g. chess
: Partially observable - sensors give it access to only partial state
	- e.g. driving

: Deterministic - next state is completely determined by current state and action
: Stochastic - next state is determined by current state, action, and randomness

: Episodic - agent's experience is divided into episodes, all action is dependent on current episodes
	- e.g. classifying images
: Sequential - current decision affects all future decisions
	- e.g. chess, driving

: Static - environment is unchanged while agent is calculating
: Dynamic - environment can change while agent is calculating
	- e.g. driving

: Discrete - finite number of actions, states, percepts
: Continuous - infinite number of actions, states, percepts

: Single Agent - agent is operating by itself
: Multi-Agent - agent is operating with other agents

Table-Lookup Driven Agent
: agent looks up what action to do per percept sequence (codon table for DNA)

Simple Reflex Agent
: do not have memory of past world states/percepts
: actions based solely on current percept
e.g. 

Model-Based Reflex Agent
: have memory of past world states/percepts
: actions based on
	- current percept
	- past world states/percepts
e.g.

Goal-Based Agent
: have memory of past world states/percepts
: actions based on 
	- current percept
	- past world states/percepts
	- goal
e.g. 

Utility-Based Agent
: have memory of past world states/percepts
: actions based on 
	- current percept
	- past world states/percepts
	- goal
	- utility function (how useful is this action to the agent and achieving its goal)
e.g. 

Learning Agent
: have memory of past world states/percepts
: actions based on 
	- current percept
	- past world states/percepts
	- goal
	- utility function (how useful is this action to the agent and achieving its goal)
	- learning function (adjust actions based on past world states/percepts)
e.g. 

Summary
: PAGE Process
	- aspects related to PAGE
		: Full observable
		: Deterministic
		: Episodic
		: Static

: agent architectures
	- table-lookup driven
	- simple reflex
	- model-based reflex
	- goal-based
	- utility-based
	- learning
--------------------------------------------
Search: The Idea
	: Tree Search - search through a tree of possible actions
		- implementation (states v nodes)
			: states - a representation of the world
			: nodes - a data structure that contains a state and other info (e.g. parent node, action, etc.)

	Search strategies
		- dimensions
			: completeness - does it always find a solution if one exists?
			: time complexity - # of nodes generated/expanded
				| b: maximum branching factor of the search tree
				| d: depth of the least-cost solution
			: space complexity - how much memory does it need?
			: optimality - does it always find the least-cost solution?

	Uninformed Search strategies - use only information available in the problem definition
		- breadth-first search: go through all nodes at a given level before going to the next level
		- depth-first search: go all the way to the lowest node, then backtrack, then go to the next lowest node, etc.
		- depth-limited search: depth-first search but with a limit on how deep you can go
			: is this not breadth-first search with a depth limit?
		- iterative deepening search: depth-limited search but with increasing depth limit
		- uniform-cost search: breadth-first search but with a cost function
			: cost function - a function that determines the cost of a node
				| e.g. cost of a node = cost of parent node + cost of action to get to current node

	Uninformed Search Summary
		- requires abstracting away real world details to define state space to be explored feasibly
		- iterative deepning search uses only linear space and not much more time than other uninformed algorithms

	A* Search
		- idea: avoid expanding paths that are already expensive
		- f(n) = g(n) + h(n)
			: g(n) = cost so far to reach n
			: h(n) = estimated cost from n to goal (heuristic, i.e. maybe use Euclidean distance to goal, not exact)
			: f(n) = estimated total cost of path through n to goal
		- admissable heuristic: never overestimates the cost to reach the goal (it is optimistic)
		=================================================
		| Theorem: if h(n) is admissable, A* is optimal |
		=================================================

	Informed Search strategies - use problem-specific knowledge to find solutions more efficiently
	--------------------------------------------

	- as the crow flies, heuristic is the hypothetical completely optimal ideal solution

	heuristic is consistent if
		- h(n) <= h(n') + c(n, a, n')
			: n is a node in the search tree
			: n' is a successor of n
			: a is the action taken to get from n to n'
			: c(n, a, n') is the step cost of taking action a from node n to node n'

	if a heuristic is consistent, it is admissable 
	if a heuristic is admissible, it does not mean it is consistent

=================================================
	Proof Exercise: Prove that if a heuristic is consistent, it must be admissible. Construct an admission heuristic that is not consistent
	(hint: use induction)
====
	let H(n) be the heuristic function, it is assumed to be consistent

	by definition of consistent, H(n) <= H(n') + c(n,a,n')
		this means that H(n)'s cost is less than or equal to the cost of H(n') + the cost of the action to get to n' from n

	and the definition of admissible states that H(n) <= H*(n)
		this means the heuristic H(n) is less than or equal to the actual cost to get to the goal from n

	since H(n') is heuristic, it must be less than or equal H*(n)
	and c(n,a,n') is the cost of the action to get to n' from n
		therefore, H(n') + c(n,a,n') <= H*(n) + c(n,a,n')
		from there, it can be concluded H(n) <= H*(n)
=================================================

Relaxed Problems
- 