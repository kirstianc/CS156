CS156

definition of AI in class; representation scheme, a search scheme, and a reasoning scheme.
	- searching, representing, reasoning

intelligent agents/systems must have
1) representation - 

wolf, goat, cabbage problem (cross river problem)
- can store information into a data structure 

- kinda like 5D chess where there are numerous outcomes that lead to more

2) search

# of planets in universe ~ 10^20
# of atoms in universe ~ 10^50
# of possible chess moves ~ 10^110 (legal moves) <-> 10^123 (illegal moves)
	- there must be some sort of algorithm or decision tree where they focus on only the useful/needed moves

- there must be some sort of search scheme that lets you find acceptable choices in a reasonable time

3) reasoning - 

imagine you have 2 buckets full of coins, run up and down the stairs, what happens to the coins? they spill out.
	- how did you figure it out? you never did this action before? 
		--> you made an analogy to an experience you had

- there must be some sort of reasoning base; deductive, inductive, something to infer what could happen

setup python environment


min-max trees and related algo
- many states with possible moves being the subsequent level (each level is a state of the environment)
  e.g. chess
	- since there is an absurd amt of possible chess moves, you only go down the tree to a certain depth

Agents and Environments --------------------
- the agent function maps from percept histories to actions [f: P* -> A]
- the agent program runs on the physical architecture to produce f
- agent = architecture + program

The PAGE Process (for designing an agent) --------------------
- PAGE = percepts, actions, goal, environment
- tasks for designing an AGENT-BASED systems
	What will the agent need to perceive?
	What actions will the agent need to perform?
	What goals will the agent need or want to accomplish? ("aim" is what you want to achieve, goal is how you achieve the "aim")
	What will be the environment in which the agent will operate?

	PAGE example (automated TAXI) --------------------
	Perceptions? - location, destination, traffic, other cars, pedestrians, traffic lights, etc.
	Actions? - accelerate, brake, turn, honk, etc.
	Goals? - get to destination, obey traffic laws, etc.
	Environment? - roads, other cars, pedestrians, traffic lights, etc.

PAGE exercise (Financial Planning Agent) --------------------
	Perceptions? current financial situation, current trends on stock markets, news channels, political climate, etc
	Actions? invest, save, purchase assets (e.g. housing, land, etc.), etc
	Goals? achieve financial stability, retirement, financial goals, etc
	Environment? emails, financial instituation dashboards, financial situations (interest rate, supply, demand), etc

--------------------------------------------
Example: Romania
- go on holiday in Romania, currently in Arad
- flight leaves tomorrow from Bucharest

Formulate Goal
	- be in Bucharest

Formulate Problem
	- states: various cities
	- actions: drive between cities
	- initial state: Arad
	- goal state: Bucharest

Find Solution
	- sequence of cities: Arad -> Sibiu -> Fagaras -> Bucharest

--------------------------------------------

Problem types
- deterministic, fully observable --> single-state problem
	: agent knows exactly which state it will be in; solution is a sequence
- deterministic, non-observable --> sensorless problem (conformat problem)
	: agent may have no idea where it is; solution is a sequence
- nondeterministic and/or partially observable --> contingency problem
	: percepts provide new information abt current state; often interleave search, execution
- unknown state space --> exploration problem
	: agent must gather data to find solution (as executing, take in new information and use it to find/adjust solution)

Tree-Search algorithm
- function TREE-SEARCH(problem, strategy) returns a solution, or failure
	- initialize the search tree using the initial state of problem
	- loop do
		- if there are no candidates for expansion then return failure
		- choose a leaf node for expansion according to strategy
		- if the node contains a goal state then return the corresponding solution
		- else expand the node and add the resulting nodes to the search tree

--------------------------------------------
Notion of Rational Agent
- an agent is an entity that perceives and acts
- rational action: whiever action maximizes the performance metric
- rationality is distinct from:
	: omniscience (knowing everything)
	: omnipotence (being able to do anything)
- agents should strive to "do the right thing", based on what they know and have perceived
	: a rational agent should do whatever it can to maximize the performance metric
		- however, the agent must do this within expectation (e.g. travel to destination but not at expense of human life, etc.)

(Performance Metric v Goal ------------------------------------------------------------------------------)
| Performance Metric - how is the agent going to reach the goal (e.g. increase portfolio val by 5% a yr) |
| Goal - what is the agent trying to achieve (e.g. financial stability)									 |
(--------------------------------------------------------------------------------------------------------)

Task Environment
: Fully observable - agent's sensors give it access to the complete state of the environment 
	- e.g. chess
: Partially observable - sensors give it access to only partial state
	- e.g. driving

: Deterministic - next state is completely determined by current state and action
: Stochastic - next state is determined by current state, action, and randomness

: Episodic - agent's experience is divided into episodes, all action is dependent on current episodes
	- e.g. classifying images
: Sequential - current decision affects all future decisions
	- e.g. chess, driving

: Static - environment is unchanged while agent is calculating
: Dynamic - environment can change while agent is calculating
	- e.g. driving

: Discrete - finite number of actions, states, percepts
: Continuous - infinite number of actions, states, percepts

: Single Agent - agent is operating by itself
: Multi-Agent - agent is operating with other agents

Table-Lookup Driven Agent
: agent looks up what action to do per percept sequence (codon table for DNA)

Simple Reflex Agent
: do not have memory of past world states/percepts
: actions based solely on current percept
e.g. 

Model-Based Reflex Agent
: have memory of past world states/percepts
: actions based on
	- current percept
	- past world states/percepts
e.g.

Goal-Based Agent
: have memory of past world states/percepts
: actions based on 
	- current percept
	- past world states/percepts
	- goal
e.g. 

Utility-Based Agent
: have memory of past world states/percepts
: actions based on 
	- current percept
	- past world states/percepts
	- goal
	- utility function (how useful is this action to the agent and achieving its goal)
e.g. 

Learning Agent
: have memory of past world states/percepts
: actions based on 
	- current percept
	- past world states/percepts
	- goal
	- utility function (how useful is this action to the agent and achieving its goal)
	- learning function (adjust actions based on past world states/percepts)
e.g. 

Summary
: PAGE Process
	- aspects related to PAGE
		: Full observable
		: Deterministic
		: Episodic
		: Static

: agent architectures
	- table-lookup driven
	- simple reflex
	- model-based reflex
	- goal-based
	- utility-based
	- learning
: advantages and disadvantages of each agent architecture
	- table-lookup driven
	pro
		:
	con
		:
	- simple reflex
	pro
		:
	con
		:
	- model-based reflex
	pro
		:
	con
		:
	- goal-based
	pro
		:
	con
		:
	- utility-based
	pro
		:
	con
		:
	- learning
	pro
		:
	con
		: