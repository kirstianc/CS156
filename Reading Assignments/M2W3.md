CS156 Module 2 Week 3 Reading
From the required textbook
1. Read chapter 2 up to and including the Summary section on page 60.

CH2: Intelligent Agents ------------------------
2.1 Agents and Environments
- Agent: anything that can perceive its environment through sensors and act upon the environment through actuators

- Percept: content the agent's sensor is detecting

- Percept sequence: complete history of all percepts detected by agent

- Agent function: maps any given percept sequence to an action (abstract mathematical description)

- Agent program: implementation of agent function (concrete computer program)


2.2 Good Behavior: The Concept of Rationality
- Rational agents "do the right thing" but how to define "right thing"?
    : consequentialism - evaluate agent's behavior by outcome
        | agent placed in env -> percept env -> agent program -> action -> env changes by action 
        | if outcome is desirable -> agent performed well
        | desirability = performance measure evaluating sequence of env states
    : better to design perf measures according to goal in env
        | e.g. vaccuum agent -> clean floor
            - if perf measure was amt of dirt, vaccuum might clean then dump dirt...
        | clean floor is subjective -> if avg cleanliness, agent could:
            - mediocre job constantly 
            - high quality job and long breaks (normal janitor)

- Rationality
    : depends on 4 things
        | performance measure
        | prior knowledge of env
        | actions agent can perform
        | percept sequence to date

- Rational Agent: for each possible percept sequence, select action (expected to maximize perf measure), given evidence provided by percept sequence and built-in knowledge
    : rationality is distinct from omniscience
        | omniscient agent knows actual outcome of actions (impossible)
        | rationality maximizes expected performance
            - e.g. if crossing street (no cars) but crushed by door from plane -> rational or not?
                : rational because it was the best action given the info available
        | raionality distinct from perfection
    : information gathering - doing actions to modify future percepts
        | e.g. looking both ways before crossing street
    : agent must learn from percepts and actions
        | e.g. if crossing street and car comes, don't cross
    : autonomy - learn what it can to compensate for partial/incorrect prior knowledge
        | e.g. bring table to door -> check room (table is moved a couple inches) -> move table back to original position -> check room (table is moved a couple inches) -> repeat


2.3 The Nature of Environments
- PEAS description (task environment)
    : Performance measure - Goal
    : Environment - Environment
    : Actuators - Actions
    : Sensors - Percepts

- PAGE description (agent environment)
    : Percepts - Sensors
    : Actions - Actuators
    : Goal - Performance measure
    : Environment - Environment

- Properties of Task Environments
    Observable - Agents - Deterministic - Episodic - Static - Discrete

    Observation (Fully, Partially, Unobservable)
    : Fully observable - sensors detect all aspects RELEVANT to the choice of action (depends on agent's goal)
    : Partially observable - sensors provide incomplete info relevant to action
    : Unobservable - no sensors at all
    
    Num of Agents (Single, Multi)
    : Single agent - agent operating by itself
    : Multiagent - multiple agents operating at the same time
    
    Determinism (Deterministic, Nondeterministic & Stochastic)
    : Deterministic - next state of env is completely determined by current state and action executed by agent
        - if env is partially observable, can appear nondeterministic
    : Nondeterministic & Stochastic - next state is not completely determined by current state and action
        - e.g. real situations with randomness
        - Nondeterministic (possibilites are not quantified) vs Stochastic (probabilities are quantified)
            | e.g. nondeterministic - "chance of rain"
            | e.g. stochastic - "30% chance of rain"
    
    Agent Experience (Episodic, Sequential)
    : Episodic - divided into atomic episodes (not affected by previous episodes)
        - e.g. assembly line robot (does not need to care for prior episodes)
    : Sequential - current decision could affect all future decisions

    Env Change (Static, Dynamic, Semidynamic)
    : Static - env doesn't change while agent is deliberating
    : Dynamic - env changes while agent deliberates
        - constantly prompting agent, if agent still deciding -> counts as doing nothing
    : Semidynamic - env doesn't change but perf score does
        - e.g. chess game, take as long as it wants to decide but perf score is based on time
    
    Time Handling (Discrete, Continuous)
    : Discrete - distinct states (e.g. chess)
    : Continuous - continuous states (e.g. driving)

    Knowledge of Env, Agent's knowledge of env (Known, Unknown)
    : Known - agent knows everything about env
        - outcomes for all actions are given
    : Unknown - agent knows nothing about env
        - agent has to learn how env works to make good decisions
    

2.4 Structure of Agents (agent = architecture + program)
- program = function mapping percept sequences to actions
- architecture = computing device with sensors and actuators

Agent Programs
- Simple reflex agents
    : select actions based on current percept
    : percept -> condition-action rule -> action

- Model-based reflex agents
    : select actions based on current percept and built-in knowledge
    : difference from simple reflex agents is that it has internal state (and models to keep track of state and how the env works/affects the state)
    : transition model - how the next state depends on the current state and action
        | e.g. turn wheel, car turns
    : sensor model - how state of world is reflected in percepts
        | e.g. when car in front is braking, 1+ red regions appear in forward camera
    > question: does a model-based reflex agent need to have both models incorporated to be considered model-based reflex agent?
    > answer: ??

- Goal-based agents
    : select actions based on current percept, built-in knowledge, and goal
    : goal - desired state of world
    : e.g. car brake lights
        | reflex agent - brake when car in front brakes
        | goal-based agent - brake when car in front brakes bc only action to achieve goal to not hit other cars

- Utility-based agents
    : select actions based on current percept, built-in knowledge, goal, and utility function
    : utility - degree of happiness
    : utility function - maps state of world to degree of happiness
        | if conflicting goals, utility function chooses appropriate action
        | if multiple goals, utility function weighs likelihood of success against importance of goals
    : expected utility - expected utility given the probabilities and utilities of each outcome
        | e.g. if 50% chance of getting $100 and 50% chance of getting $0, expected utility is $50

- Learning agents
    : learning element - responsible for making improvements
        | takes feedback from critic and decides how to improve performance element
    : performance element - selecting external actions
        | takes in percepts and decides actions
    : critic - 
    : problem generator - 

2. Read chapter 3 up to and including the Summary section on page 104-106.


The above reading assignment is expected to be completed over a two week period. That is, complete reading assignment 1 during the first of the two weeks, and complete reading assignment 2 during the second week.
