CS156 Module 2 Week 3 Reading
From the required textbook
1. Read chapter 2 up to and including the Summary section on page 60.

CH2: Intelligent Agents ------------------------
2.1 Agents and Environments
- Agent: anything that can perceive its environment through sensors and act upon the environment through actuators

- Percept: content the agent's sensor is detecting

- Percept sequence: complete history of all percepts detected by agent

- Agent function: maps any given percept sequence to an action (abstract mathematical description)

- Agent program: implementation of agent function (concrete computer program)


2.2 Good Behavior: The Concept of Rationality
- Rational agents "do the right thing" but how to define "right thing"?
    : consequentialism - evaluate agent's behavior by outcome
        | agent placed in env -> percept env -> agent program -> action -> env changes by action 
        | if outcome is desirable -> agent performed well
        | desirability = performance measure evaluating sequence of env states
    : better to design perf measures according to goal in env
        | e.g. vaccuum agent -> clean floor
            - if perf measure was amt of dirt, vaccuum might clean then dump dirt...
        | clean floor is subjective -> if avg cleanliness, agent could:
            - mediocre job constantly 
            - high quality job and long breaks (normal janitor)

- Rationality
    : depends on 4 things
        | performance measure
        | prior knowledge of env
        | actions agent can perform
        | percept sequence to date

- Rational Agent: for each possible percept sequence, select action (expected to maximize perf measure), given evidence provided by percept sequence and built-in knowledge
    : rationality is distinct from omniscience
        | omniscient agent knows actual outcome of actions (impossible)
        | rationality maximizes expected performance
            - e.g. if crossing street (no cars) but crushed by door from plane -> rational or not?
                : rational because it was the best action given the info available
        | raionality distinct from perfection
    : information gathering - doing actions to modify future percepts
        | e.g. looking both ways before crossing street
    : agent must learn from percepts and actions
        | e.g. if crossing street and car comes, don't cross
    : autonomy - learn what it can to compensate for partial/incorrect prior knowledge
        | e.g. bring table to door -> check room (table is moved a couple inches) -> move table back to original position -> check room (table is moved a couple inches) -> repeat


2.3 The Nature of Environments
- PEAS description (task environment)
    : Performance measure - Goal
    : Environment - Environment
    : Actuators - Actions
    : Sensors - Percepts

- PAGE description (agent environment)
    : Percepts - Sensors
    : Actions - Actuators
    : Goal - Performance measure
    : Environment - Environment

- Properties of Task Environments
    Observable - Agents - Deterministic - Episodic - Static - Discrete

    Observation (Fully, Partially, Unobservable)
    : Fully observable - sensors detect all aspects RELEVANT to the choice of action (depends on agent's goal)
    : Partially observable - sensors provide incomplete info relevant to action
    : Unobservable - no sensors at all
    
    Num of Agents (Single, Multi)
    : Single agent - agent operating by itself
    : Multiagent - multiple agents operating at the same time
    
    Determinism (Deterministic, Nondeterministic & Stochastic)
    : Deterministic - next state of env is completely determined by current state and action executed by agent
        - if env is partially observable, can appear nondeterministic
    : Nondeterministic & Stochastic - next state is not completely determined by current state and action
        - e.g. real situations with randomness
        - Nondeterministic (possibilites are not quantified) vs Stochastic (probabilities are quantified)
            | e.g. nondeterministic - "chance of rain"
            | e.g. stochastic - "30% chance of rain"
    
    Agent Experience (Episodic, Sequential)
    : Episodic - divided into atomic episodes (not affected by previous episodes)
        - e.g. assembly line robot (does not need to care for prior episodes)
    : Sequential - current decision could affect all future decisions

    Env Change (Static, Dynamic, Semidynamic)
    : Static - env doesn't change while agent is deliberating
    : Dynamic - env changes while agent deliberates
        - constantly prompting agent, if agent still deciding -> counts as doing nothing
    : Semidynamic - env doesn't change but perf score does
        - e.g. chess game, take as long as it wants to decide but perf score is based on time
    
    Time Handling (Discrete, Continuous)
    : Discrete - distinct states (e.g. chess)
    : Continuous - continuous states (e.g. driving)

    Knowledge of Env, Agent's knowledge of env (Known, Unknown)
    : Known - agent knows everything about env
        - outcomes for all actions are given
    : Unknown - agent knows nothing about env
        - agent has to learn how env works to make good decisions
    

2.4 Structure of Agents (agent = architecture + program)
- program = function mapping percept sequences to actions
- architecture = computing device with sensors and actuators

Agent Programs
- Simple reflex agents
    : select actions based on current percept
    : percept -> condition-action rule -> action

- Model-based reflex agents
    : select actions based on current percept and built-in knowledge
    : difference from simple reflex agents is that it has internal state (and models to keep track of state and how the env works/affects the state)
    : transition model - how the next state depends on the current state and action
        | e.g. turn wheel, car turns
    : sensor model - how state of world is reflected in percepts
        | e.g. when car in front is braking, 1+ red regions appear in forward camera
    > question: does a model-based reflex agent need to have both models incorporated to be considered model-based reflex agent?
    > answer: ??

- Goal-based agents
    : select actions based on current percept, built-in knowledge, and goal
    : goal - desired state of world
    : e.g. car brake lights
        | reflex agent - brake when car in front brakes
        | goal-based agent - brake when car in front brakes bc only action to achieve goal to not hit other cars

- Utility-based agents
    : select actions based on current percept, built-in knowledge, goal, and utility function
    : utility - degree of happiness
    : utility function - maps state of world to degree of happiness
        | if conflicting goals, utility function chooses appropriate action
        | if multiple goals, utility function weighs likelihood of success against importance of goals
    : expected utility - expected utility given the probabilities and utilities of each outcome
        | e.g. if 50% chance of getting $100 and 50% chance of getting $0, expected utility is $50

- Learning agents
    : learning element - responsible for making improvements
        | takes feedback from critic and decides how to improve performance element
    : performance element - selecting external actions
        | takes in percepts and decides actions
    : critic - provides feedback to learning element
        | e.g. teacher, reward function
    : problem generator - suggests actions that will lead to new/informative experiences
    : reward/penalty - provides feedback on quality of agent's action

- Agent Programs
    Atomic Representation
    : each state in the world is indivisible (no internal structure)
        | e.g. city problem - each state is the name of a city
    
    Factored Representation
    : splits up each state into fixed set of variables/attributes
        | e.g. city problem - need to know more than just the name of the city
            - how much gas in tank, current GPS coords, how much money we have for tolls

    Structured Representation
    : allows for attributes to have relations to each other
        | e.g. city problem - downtown is closed for parade
            - downtown is closed for parade -> other streets close by will be closed
    
    Expressiveness
    : more expressive = more concise
        | e.g. explaining chess
            - 1-2 pages for structured representation (relations between pieces) 
            - 1000+ pages for atomic representation (each possible move in every state possible is described)
    
    Localist Representation
    : 1-1 mapping between concepts and memory locations
        | e.g. city problem - each city is represented by a memory location
            - if city is closed, memory location is set to 1
            - if city is open, memory location is set to 0

    Distributed Representation
    : concept spread over many memory locations and each memory location is used as representation of other mulitple different concepts

Summary
- agent = something that perceives and acts in an environment
- agent function = specifies action to be taken by agent in response to percept sequence
- performance measure = evaluates agent behavior in environment
- rational agent = acts to maximize performance measure, given percept sequence
- task environment = PEAS description; performance measure, environment, actions, sensors
    : can be fully/partially observable, single/multi agent, deterministic/nondeterministic, episodic/sequential, static/dynamic/semidynamic, discrete/continuous, known/unknown
- significant risk if agent optimizes wrong objective (need to reflect uncertainty abt true objective)
- agent program implements agent function
- simple reflex agent = select action based on current percept
- model-based reflex agent = select action based on current percept, past percept, and how the action will impact the environment
- goal-based agent = select action based on current percept, past percept, how the action will impact the environment, and goal
- utility-based agent = select action based on current percept, past percept, how the action will impact the environment, goal, and utility function
- all agents improve performance through learning

2. Read chapter 3 up to and including the Summary section on page 104-106.
Problem Solving Agents - consider sequence of actions that form path to goal state
    - use atomic representation (consider world as wholes)
Search - computation used in a problem solving agent ^
Planning Agents - use factored or structured representations

3.1 Problem-Solving Agents
- when environment is unknown, best action for agent is typically random

Problem-Solving Process
1. Goal formulation
    : define goal in terms of current state
    : e.g. goal is to get to Bucharest
2. Problem formulation
    : define problem in terms of current state, actions, and goal
    : e.g. problem is to find a route to Bucharest
3. Search
    : find sequence of actions that lead from current state to goal state (solution)
    : e.g. find route from Arad to Bucharest
4. Execution
    : execute actions in sequence found by search (one at a time)
    : e.g. go to Bucharest

- in a fully observable, deterministic, known environment -> solution is a fixed sequence of actions

Open-Loop system
    : no feedback from environment, ignore percepts

Closed-Loop system
    : feedback from environment, use percepts

Search Problem can be defined as:
- state space (set of all possible states of environment)
- initial state (start state)
- goal state (can have more than one, could be property applied to states)
- actions available to agent
- transition model (describes what each action does)
- action cost function (cost of each action)

Path/Solution
    : sequence of actions from initial state to goal state